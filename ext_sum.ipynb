{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import triu\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import doc2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "The field of natural language processing (NLP) has seen significant advancements in recent years. Researchers are developing new algorithms and models to improve the understanding and generation of human language. One notable development is the creation of word embeddings, which represent words as dense vectors in a continuous vector space. This representation allows for capturing semantic relationships between words.\n",
    "\n",
    "Word2Vec is a popular method for generating word embeddings. It uses a neural network to learn word associations from a large corpus of text. The resulting vectors can be used in various NLP tasks, such as text classification, sentiment analysis, and machine translation.\n",
    "\n",
    "In extractive summarization, the goal is to select key sentences from a document that best represent its content. By using word embeddings, we can identify sentences that are semantically similar to a set of predefined keywords. This approach helps in generating summaries that are both relevant and concise.\n",
    "\n",
    "Overall, the integration of word embeddings into NLP tasks has opened new possibilities for developing more sophisticated language models. These advancements are paving the way for better human-computer interaction and more effective information retrieval systems.\n",
    "\n",
    "Sneha Sathish Kumar \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split text into sentences\n",
    "sentences = [sentence.strip() for sentence in text.split('\\n') if sentence]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "\n",
    "# Train Word2Vec model on the tokenized sentences\n",
    "model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Define keywords\n",
    "keywords = [\"natural language processing\", \"word embeddings\", \"Word2Vec\", \"NLP tasks\", \"Sneha Sathish Kumar\"]\n",
    "\n",
    "# Tokenize and vectorize keywords\n",
    "keyword_vectors = [model.wv[keyword.split()] for keyword in keywords]\n",
    "keyword_vector = np.mean(np.array([np.mean(vectors, axis=0) for vectors in keyword_vectors]), axis=0)\n",
    "\n",
    "# Compute sentence vectors\n",
    "sentence_vectors = [np.mean([model.wv[word] for word in sentence if word in model.wv], axis=0) for sentence in tokenized_sentences]\n",
    "\n",
    "# Compute cosine similarity between sentence vectors and keyword vector\n",
    "similarities = [cosine_similarity([vector], [keyword_vector])[0][0] for vector in sentence_vectors]\n",
    "\n",
    "# Rank sentences by similarity\n",
    "ranked_sentences = sorted(((similarity, sentence) for similarity, sentence in zip(similarities, sentences)), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select top N sentences (let's say 3 for this example)\n",
    "top_n = 3\n",
    "summary_sentences = sorted([sentence for _, sentence in ranked_sentences[:top_n]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sneha Sathish Kumar The field of natural language processing (NLP) has seen significant advancements in recent years. Researchers are developing new algorithms and models to improve the understanding and generation of human language. One notable development is the creation of word embeddings, which represent words as dense vectors in a continuous vector space. This representation allows for capturing semantic relationships between words. Word2Vec is a popular method for generating word embeddings. It uses a neural network to learn word associations from a large corpus of text. The resulting vectors can be used in various NLP tasks, such as text classification, sentiment analysis, and machine translation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate summary\n",
    "summary = \" \".join(summary_sentences)\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
